{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4f6682e7-14b9-446d-b36b-e7239e0d0676",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "import gymnasium as gym\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "80a61043-8a9c-416a-9fb8-3563d0e94aab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.29.1'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gym.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ceca0153-2b33-4f8a-83b5-4845ace26831",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('Blackjack-v1', natural=False, sab=False, render_mode='rgb_array')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d5c1bf4c-a519-4145-8f0e-2577d6e2bd2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space.sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f90616e-a154-4956-80fa-27a5554c27af",
   "metadata": {},
   "source": [
    "# 作业一：将离线改成在线"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f55d93a4-eae8-4a31-af3b-5518ce135b37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4102\n"
     ]
    }
   ],
   "source": [
    "\n",
    "episode = 10000\n",
    "Q_table = defaultdict(lambda: [0, 0])  # 0th, action-0, state: value, action-1, state: value\n",
    "\n",
    "win_or_loss = []\n",
    "\n",
    "\n",
    "def train(Q_table: defaultdict[lambda: [0, 0]], trajectory: (int, int, int)):\n",
    "    gamma = 0.99\n",
    "    s, a, r = trajectory\n",
    "    if not Q_table[s][a]:\n",
    "        Q_table[s][a] = 0\n",
    "    old_G = Q_table[s][a]\n",
    "    Q_table[s][a] = np.mean([old_G, gamma * r])\n",
    "\n",
    "\n",
    "def policy(Q_table, state):\n",
    "    if Q_table[state]:\n",
    "        return np.argmax(Q_table[state])\n",
    "    else:\n",
    "        return env.action_space.sample()\n",
    "\n",
    "\n",
    "for _ in range(episode):\n",
    "    state, info = env.reset()\n",
    "    while True:\n",
    "        action = policy(Q_table, state)\n",
    "        state_next, reward, terminated, truncated, info = env.step(action)\n",
    "        # (s, a, r)\n",
    "        trajectory = (state, action, reward)\n",
    "\n",
    "        state = state_next\n",
    "\n",
    "        if terminated or truncated:\n",
    "            train(Q_table, trajectory)\n",
    "            if reward == 1:\n",
    "                win_or_loss.append(1)\n",
    "            else:\n",
    "                win_or_loss.append(0)\n",
    "            break\n",
    "env.close()\n",
    "# %%\n",
    "\n",
    "print(sum(win_or_loss) / len(win_or_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d0e8ba7-2780-4caf-93c0-57be9f041900",
   "metadata": {},
   "source": [
    "在线得方式会实时更新计算得Q值，模型最开始很差，但是会逐渐一点一点得变好。速度会快，但是批量off-line得更新方式因为计算mean得时候会更准确，精度更高"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83ac6434",
   "metadata": {},
   "source": [
    "### response - 01\n",
    "\n",
    "是的，你的代码没问题，而且对这个方法的问题也理解的没有问题。 \n",
    "\n",
    "我们可以对代码做一点点的小小的变化，让它不至于刚开始的时候非常执着于错误的方向：\n",
    "\n",
    "```python \n",
    "\n",
    "def policy(Q_table, state):\n",
    "    eps = 0.05\n",
    "    if random.random() < eps or state not in Q_table: \n",
    "        return env.action_space.sample()\n",
    "    else:\n",
    "        return np.argmax(Q_table[state])\n",
    "    \n",
    "这个在RL中是一个非常典型的问题，叫做\"exploration vs exploitation\" "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "274b2639-5e15-4aef-b3d6-88b19c6badaf",
   "metadata": {},
   "source": [
    "# 作业二\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39eecfb7-6b58-4da7-8913-afd617a4fcaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "我们可以修改得地方\n",
    "1. 修改奖励机制，对r的值进行变化\n",
    "2. 调整折扣因子gamma\n",
    "3. 增加训练次数和样本量episode得值\n",
    "4. 调整policy策略"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e386d81e-d6b1-44f2-93c3-287147cc42fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "可以增加剩余卡牌队中高点数和低点数的比例可以进行学习"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0fe14f0",
   "metadata": {},
   "source": [
    "### response-02\n",
    "\n",
    "以上思路没有问题，不过如果你想知道具体哪种方法更有效，可以自己写代码实现以下，观察一下真实的结果~ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3523626b-255b-4175-82d2-53a6407c743b",
   "metadata": {},
   "source": [
    "# 作业三"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a4f42b0-6505-488c-a137-f9a68edcdd12",
   "metadata": {},
   "source": [
    "强化学习是没有绝对的对错，只是一种不同选择带来不同的后果中进行学习【具有反馈机制】，但是监督学习是在有明确的答案基础上，向标准答案学习的方式。\n",
    "非监督学习是自己找出标准答案，发现数据的特征。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb94794d",
   "metadata": {},
   "source": [
    "### response-03\n",
    "\n",
    "强化学习其实也有“决定的对错“，例如在某个state下，是存在一个最优动作的，但是ve'ge"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
